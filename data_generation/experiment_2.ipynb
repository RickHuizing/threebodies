{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from data_generation import Three_body_2D_Rick\n",
    "from data_generation import tbp_util\n",
    "from data_generation.tbp_energy_calculations import visualize_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment 2\n",
    "\n",
    "Same as experiment 1 but with 1 change:\n",
    "- predict delta's\n",
    "\n",
    "sweep learning rates?\n",
    "Up layers and neurons?\n",
    "Go back to original question:\n",
    "- can the network predict t=10 just as well as t0.1=100 just as well ass t10e-5=10e6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"Experiment_2\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 10,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 20,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 1,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2_1\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 10,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 20,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 2,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "experiment_id = \"Experiment_2_1\"\n",
    "config_name = experiments[experiment_id][\"dataset\"][\"name\"]\n",
    "tbp_util.use_config(config_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# allow autoreloading of imported modules whenever running a cell\n",
    "# if not included, a kernel restart is needed whenever one of the imports is modified\n",
    "# (Needs experimenting / import statements to work)\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo read from config file\n",
    "# also see tbp_util.py\n",
    "G = 1.0\n",
    "M = np.array([1.0, 1.0, 1.0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Choose example trajectory from the data set to visualize."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example_dataset = \"325\"\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "visualize_dataset(*tbp_util.load_dataset(example_dataset), G, M)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x, y, vx, vy = tbp_util.load_dataset(example_dataset)\n",
    "x[0, :]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y[0, :]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# deltas"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_deltas(x, y, vx, vy, delta=1, scaling_factor=1):\n",
    "    dx = (x[:-delta] - x[delta:]) * scaling_factor\n",
    "    dy = (y[:-delta] - y[delta:]) * scaling_factor\n",
    "    dvx = (vx[:-delta] - vx[delta:]) * scaling_factor\n",
    "    dvy = (vy[:-delta] - vy[delta:]) * scaling_factor\n",
    "    return dx, dy, dvx, dvy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "deltas = get_deltas(x, y, vx, vy, 1, 100000)\n",
    "plt.figure()\n",
    "plt.boxplot(deltas[0], showfliers=False)\n",
    "plt.legend((\"x1 delta\", \"x2 delta\", \"x3 delta\"))\n",
    "plt.show()\n",
    "plt.figure()\n",
    "plt.boxplot(deltas[2], showfliers=False)\n",
    "plt.legend((\"vx1 delta\", \"vx2 delta\", \"vx3 delta\"))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.backend.set_floatx('float64')\n",
    "keras.backend.floatx()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load training data\n",
    "\n",
    "Load training data.\n",
    "A trajectory has t timesteps. For each time step it has 12 variables:\n",
    "- x, y, vx, vy for each of the 3 bodies\n",
    "\n",
    "max_datasets: maximum number of datasets to load and train on\n",
    "prediction offset: how many time steps to predict into the future\n",
    "downsample factor: only use one in downsample_factor data points for training, allows for k-partition validation\n",
    "dataset_index: if using a downsample factor of 10, there are 10 unique sub-datasets that can be used"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_datasets = experiments[experiment_id]['max_datasets']\n",
    "prediction_offset = experiments[experiment_id]['prediction_offset']\n",
    "downsample_factor = experiments[experiment_id]['dataset']['downsample_factor']\n",
    "dataset_index = experiments[experiment_id]['dataset']['dataset_index']\n",
    "scaling_factor = experiments[experiment_id]['dataset']['delta_scaling_factor']\n",
    "\n",
    "x_train = np.ndarray((0, 12), dtype=np.float64)\n",
    "y_train = np.ndarray((0, 12), dtype=np.float64)\n",
    "for dataset, x, y, vx, vy in tbp_util.load_datasets(limit=max_datasets):\n",
    "    x = x[dataset_index:, :]\n",
    "    y = y[dataset_index:, :]\n",
    "    vx = vx[dataset_index:, :]\n",
    "    vy = vy[dataset_index:, :]\n",
    "\n",
    "    input_data = np.column_stack((x, y, vx, vy))\n",
    "    input_data = input_data[:-prediction_offset:downsample_factor, :]\n",
    "\n",
    "    deltas = get_deltas(x, y, vx, vy, delta=prediction_offset, scaling_factor=scaling_factor)\n",
    "    output_data = np.column_stack(deltas)[::downsample_factor, :]\n",
    "\n",
    "    x_train = np.concatenate((x_train, input_data))\n",
    "    y_train = np.concatenate((y_train, output_data))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "assert x_train.shape == y_train.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "first training example **[ x1, x2, x3, y1, y2, y3, vx1, vx2, vx3, vy1, vy2, vy3 ]**\n",
    "first testing  example **[ x1', x2', x3', y1', y2', y3', vx1', vx2', vx3', vy1', vy2', vy3' ]**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train[0,]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train[0,]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize model\n",
    "\n",
    "The model consists of 10 layers with 128 neurons. Initial testing has shown this model to be able to over-fit.\n",
    "The layers use a ReLU activation function and the 12 output neurons a linear function.\n",
    "\n",
    "The model has be trained on batches of size 256. 10% of the data set is set aside for validation.\n",
    "\n",
    "This set-up was used to do a preliminary search for network architecture:\n",
    "- learning rate\n",
    "- n layers, neurons, batch size, loss function\n",
    "\n",
    "# Fit model\n",
    "After every epoch, the model is automatically backed-up, so you can interrupt the kernel and go to validation steps during training if you're curious! Then just re-run the model.fit() step to continue training (might need some testing, lost the loss curve before)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = experiments[experiment_id]['batch_size']\n",
    "epochs = experiments[experiment_id]['epochs']\n",
    "validation_split = experiments[experiment_id]['validation_split']\n",
    "learning_rate = experiments[experiment_id]['learning_rate']\n",
    "steps_per_epoch = round((x_train.shape[0] * (1 - validation_split)) / batch_size)\n",
    "\n",
    "print(f\"{batch_size} {epochs} {validation_split} {learning_rate}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_model() -> keras.models.Sequential:\n",
    "    neurons = experiments[experiment_id]['neurons_per_layer']\n",
    "    no_dense_layers = experiments[experiment_id]['no_dense_layers']\n",
    "    # start with input layer\n",
    "    layers = [keras.layers.Dense(neurons, activation=keras.activations.relu, input_shape=[12])]\n",
    "    # add dense layers\n",
    "    layers.extend([keras.layers.Dense(neurons, activation=keras.activations.relu) for _ in range(no_dense_layers - 1)])\n",
    "    # add output layer\n",
    "    layers.append(keras.layers.Dense(12, activation=keras.activations.linear)\n",
    "                  )\n",
    "\n",
    "    return keras.Sequential(layers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(\n",
    "    keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss='mae',\n",
    "    metrics=['mae', 'mse']\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hist_callback = keras.callbacks.History()\n",
    "callbacks = [\n",
    "    hist_callback,\n",
    "    keras.callbacks.BackupAndRestore(backup_dir=\"model_backup\")\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save model for later\n",
    "\n",
    "And plot of the loss as the epochs progress."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_id = f'{experiment_id}-{config_name}'\n",
    "model_path = f'./models/{experiment_id}/{config_name}'\n",
    "os.makedirs(model_path, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(f\"{model_path}/{model_id}_model\")\n",
    "\n",
    "for x in [\"h5\"]:\n",
    "    model.save(f\"{model_path}/{model_id}_model.{x}\", save_format=x)\n",
    "\n",
    "for x in [\"h5\", \"tf\"]:\n",
    "    model.save_weights(f\"{model_path}/{model_id}_weights.{x}\", save_format=x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.makedirs(f'{model_path}/{model_id}')\n",
    "plt.figure()\n",
    "plt.title(f\"Loss graph for {experiment_id}\")\n",
    "plt.plot(hist_callback.history['loss'], label=\"loss\")\n",
    "plt.savefig(f'{model_path}/{model_id}/loss.svg', format='svg', dpi=1200)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_path"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'{model_path}/{model_id}/loss.json', 'w') as f:\n",
    "    f.write(json.dumps(hist_callback.history))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Quick validation set-up\n",
    "\n",
    "Choose a trajectory from the dataset.\n",
    "Visualize trajectory\n",
    "use model to predict the same trajectory ( but in fewer steps! )\n",
    "compare the 2 trajectory plots\n",
    "# todo compare the loss or something\n",
    "# todo save results of validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset_to_predict = '72'\n",
    "dataset_to_predict = '18'\n",
    "x, y, vx, vy = tbp_util.load_dataset(dataset_to_predict)\n",
    "print(x.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "length_to_predict = int(x.shape[0] / prediction_offset)\n",
    "print(\n",
    "    f\"The original trajectory is T={x.shape[0]} time steps long, so we have to predict T/prediction_offset={x.shape[0]}/{prediction_offset}={length_to_predict} steps because we predict {prediction_offset} steps into the future.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "limit = length_to_predict\n",
    "y_pred = np.zeros((limit, 12), dtype=np.float64)\n",
    "y_pred[0,] = np.concatenate((x[0,], y[0,], vx[0,], vy[0,]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(limit - 1):\n",
    "    prediction = model(y_pred[i,].reshape(1, 12), training=False).numpy()\n",
    "\n",
    "    # convert the delta's to an actual prediction\n",
    "    prediction /= scaling_factor\n",
    "    prediction = y_pred[i,].reshape(1, 12) - prediction\n",
    "\n",
    "    # stop early when the system gets out of bounds\n",
    "    if np.min(prediction[0, :6]) < -3 or np.max(prediction[0, :6]) > 3 or np.min(prediction) < -20 or np.max(\n",
    "            prediction) > 20:\n",
    "        print(f\"Stop predicting at t={i * prediction_offset} ({i} steps) after encountering {prediction}\")\n",
    "        break\n",
    "\n",
    "    y_pred[i + 1,] = prediction\n",
    "\n",
    "y_pred2 = y_pred[:i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Real trajectory\n",
    "visualize_dataset(*tbp_util.load_dataset(dataset_to_predict), G, M, down_sample_factor=prediction_offset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predicted trajectory\n",
    "pred_x, pred_y, pred_vx, pred_vy = np.hsplit(y_pred, 4)\n",
    "visualize_dataset(pred_x, pred_y, pred_vx, pred_vy, G, M, down_sample_factor=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import Three_body_2D_Rick\n",
    "\n",
    "# Comparison plot\n",
    "true_x, true_y, _, _ = tbp_util.load_dataset(dataset_to_predict)\n",
    "Three_body_2D_Rick.compare_plot(true_x, true_y, pred_x, pred_y,path=f'{model_path}/{model_id}', savefig=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
