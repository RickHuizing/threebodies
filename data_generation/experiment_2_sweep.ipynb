{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-02T00:49:07.423914400Z",
     "start_time": "2023-07-02T00:49:07.389968500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'float64'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from data_generation import Three_body_2D_Rick\n",
    "from data_generation import tbp_util\n",
    "from data_generation.tbp_energy_calculations import visualize_dataset\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "keras.backend.set_floatx('float64')\n",
    "keras.backend.floatx()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment 2\n",
    "\n",
    "Same as experiment 1 but with 1 change:\n",
    "- predict delta's\n",
    "\n",
    "sweep learning rates?\n",
    "Up layers and neurons?\n",
    "Go back to original question:\n",
    "- can the network predict t=10 just as well as t0.1=100 just as well ass t10e-5=10e6\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "experiments = {\n",
    "\n",
    "    \"Experiment_0\": {\n",
    "        \"max_datasets\": 63,\n",
    "        \"prediction_offset\": 10,  # predict t time steps ahead\n",
    "        \"batch_size\": 64,\n",
    "        \"epochs\": 3,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 1,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 10,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 80,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 1,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2_1\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 10,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 80,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 2,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2_2\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 100,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 80,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 3,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2_3\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 100,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 80,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.00001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 4,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2_4\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 100,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 80,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 256,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 5,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2_5\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 100,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 80,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 16,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 6,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2_6\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 100,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 80,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 16,\n",
    "        \"neurons_per_layer\": 128,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"breen-et-al-00001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 10,\n",
    "            \"dataset_index\": 7,\n",
    "            \"delta_scaling_factor\": 10000\n",
    "        }\n",
    "    },\n",
    "    \"Experiment_2_7\": {\n",
    "        \"max_datasets\": 102,\n",
    "        \"prediction_offset\": 1000,  # predict t time steps ahead\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 80,\n",
    "        \"validation_split\": 0.1,\n",
    "        \"no_dense_layers\": 10,\n",
    "        \"neurons_per_layer\": 256,\n",
    "        \"learning_rate\": 0.0001,\n",
    "        \"dataset\": {\n",
    "            \"name\": \"verlet-000001\",\n",
    "            # filter 1 in 10 values to reduce RAM usage\n",
    "            \"downsample_factor\": 50,\n",
    "            \"dataset_index\": 8,\n",
    "            \"delta_scaling_factor\": 100000\n",
    "        }\n",
    "    }\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T00:49:07.452368700Z",
     "start_time": "2023-07-02T00:49:07.422416400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# todo read from config file\n",
    "# also see tbp_util.py\n",
    "G = 1.0\n",
    "M = np.array([1.0, 1.0, 1.0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T00:49:07.483318800Z",
     "start_time": "2023-07-02T00:49:07.425412Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting Experiment_0\n",
      "\n",
      "Setting breen-et-al-00001 as the configuration to load trajectories from\n",
      "load 0\n",
      "load 10\n",
      "load 101\n",
      "load 102\n",
      "load 104\n",
      "load 105\n",
      "load 11\n",
      "load 114\n",
      "load 12\n",
      "load 122\n",
      "load 125\n",
      "load 128\n",
      "load 135\n",
      "load 139\n",
      "load 145\n",
      "load 158\n",
      "load 159\n",
      "load 162\n",
      "load 165\n",
      "load 166\n",
      "load 167\n",
      "load 17\n",
      "load 170\n",
      "load 172\n",
      "load 173\n",
      "load 175\n",
      "load 179\n",
      "load 185\n",
      "load 186\n",
      "load 20\n",
      "load 22\n",
      "load 26\n",
      "load 28\n",
      "load 29\n",
      "load 32\n",
      "load 33\n",
      "load 35\n",
      "load 37\n",
      "load 4\n",
      "load 52\n",
      "load 53\n",
      "load 55\n",
      "load 57\n",
      "load 58\n",
      "load 59\n",
      "load 66\n",
      "load 67\n",
      "load 68\n",
      "load 69\n",
      "load 7\n",
      "load 70\n",
      "load 74\n",
      "load 76\n",
      "load 8\n",
      "load 85\n",
      "load 86\n",
      "load 89\n",
      "load 9\n",
      "load 90\n",
      "load 92\n",
      "load 93\n",
      "load 97\n",
      "load 99\n",
      "loaded 0 datasets\n",
      "64 3 0.1 0.0001\n",
      "Epoch 1/3\n",
      "14553/22140 [==================>...........] - ETA: 36s - loss: 33.1677 - mae: 33.1677 - mse: 521384.2065"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for experiment_id in experiments.keys():\n",
    "    print(f'starting {experiment_id}\\n')\n",
    "\n",
    "    config_name = experiments[experiment_id][\"dataset\"][\"name\"]\n",
    "    tbp_util.use_config(config_name)\n",
    "\n",
    "\n",
    "    def get_deltas(x, y, vx, vy, delta=1, scaling_factor=1):\n",
    "        dx = (x[:-delta] - x[delta:]) * scaling_factor\n",
    "        dy = (y[:-delta] - y[delta:]) * scaling_factor\n",
    "        dvx = (vx[:-delta] - vx[delta:]) * scaling_factor\n",
    "        dvy = (vy[:-delta] - vy[delta:]) * scaling_factor\n",
    "        return dx, dy, dvx, dvy\n",
    "\n",
    "\n",
    "    max_datasets = experiments[experiment_id]['max_datasets']\n",
    "    prediction_offset = experiments[experiment_id]['prediction_offset']\n",
    "    downsample_factor = experiments[experiment_id]['dataset']['downsample_factor']\n",
    "    dataset_index = experiments[experiment_id]['dataset']['dataset_index']\n",
    "    scaling_factor = experiments[experiment_id]['dataset']['delta_scaling_factor']\n",
    "\n",
    "    x_train = np.ndarray((0, 12), dtype=np.float64)\n",
    "    y_train = np.ndarray((0, 12), dtype=np.float64)\n",
    "    for dataset, x, y, vx, vy in tbp_util.load_datasets(limit=max_datasets):\n",
    "        x = x[dataset_index:, :]\n",
    "        y = y[dataset_index:, :]\n",
    "        vx = vx[dataset_index:, :]\n",
    "        vy = vy[dataset_index:, :]\n",
    "\n",
    "        input_data = np.column_stack((x, y, vx, vy))\n",
    "        input_data = input_data[:-prediction_offset:downsample_factor, :]\n",
    "\n",
    "        deltas = get_deltas(x, y, vx, vy, delta=prediction_offset, scaling_factor=scaling_factor)\n",
    "        output_data = np.column_stack(deltas)[::downsample_factor, :]\n",
    "\n",
    "        x_train = np.concatenate((x_train, input_data))\n",
    "        y_train = np.concatenate((y_train, output_data))\n",
    "    assert x_train.shape == y_train.shape\n",
    "\n",
    "    batch_size = experiments[experiment_id]['batch_size']\n",
    "    epochs = experiments[experiment_id]['epochs']\n",
    "    validation_split = experiments[experiment_id]['validation_split']\n",
    "    learning_rate = experiments[experiment_id]['learning_rate']\n",
    "    steps_per_epoch = round((x_train.shape[0] * (1 - validation_split)) / batch_size)\n",
    "\n",
    "    print(f\"{batch_size} {epochs} {validation_split} {learning_rate}\")\n",
    "\n",
    "\n",
    "    def create_model() -> keras.models.Sequential:\n",
    "        neurons = experiments[experiment_id]['neurons_per_layer']\n",
    "        no_dense_layers = experiments[experiment_id]['no_dense_layers']\n",
    "        # start with input layer\n",
    "        layers = [keras.layers.Dense(neurons, activation=keras.activations.relu, input_shape=[12])]\n",
    "        # add dense layers\n",
    "        layers.extend(\n",
    "            [keras.layers.Dense(neurons, activation=keras.activations.relu) for _ in range(no_dense_layers - 1)])\n",
    "        # add output layer\n",
    "        layers.append(keras.layers.Dense(12, activation=keras.activations.linear)\n",
    "                      )\n",
    "\n",
    "        return keras.Sequential(layers)\n",
    "\n",
    "\n",
    "    model = create_model()\n",
    "    model.compile(\n",
    "        keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='mae',\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "\n",
    "    hist_callback = keras.callbacks.History()\n",
    "    callbacks = [\n",
    "        hist_callback,\n",
    "        keras.callbacks.BackupAndRestore(backup_dir=f\"model_backup/{experiment_id}\")\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_split=validation_split,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # save model for later\n",
    "    model_id = f'{experiment_id}-{config_name}'\n",
    "    model_path = f'./models/{experiment_id}/{config_name}'\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "    model.save(f\"{model_path}/{model_id}_model\")\n",
    "\n",
    "    for x in [\"h5\"]:\n",
    "        model.save(f\"{model_path}/{model_id}_model.{x}\", save_format=x)\n",
    "\n",
    "    for x in [\"h5\", \"tf\"]:\n",
    "        model.save_weights(f\"{model_path}/{model_id}_weights.{x}\", save_format=x)\n",
    "\n",
    "    os.makedirs(f'{model_path}/{model_id}', exist_ok=True)\n",
    "    plt.figure()\n",
    "    plt.title(f\"Loss graph for {experiment_id}\")\n",
    "    plt.plot(hist_callback.history['loss'], label=\"loss\")\n",
    "    plt.savefig(f'{model_path}/{model_id}/loss.svg', format='svg', dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    import json\n",
    "\n",
    "    with open(f'{model_path}/{model_id}/loss.json', 'w') as f:\n",
    "        f.write(json.dumps(hist_callback.history))\n",
    "\n",
    "    # validation\n",
    "    dataset_to_predict = '18'\n",
    "    dataset_to_predict = '17'\n",
    "    x, y, vx, vy = tbp_util.load_dataset(dataset_to_predict)\n",
    "\n",
    "    length_to_predict = int(x.shape[0] / prediction_offset)\n",
    "    print(\n",
    "        f\"The original trajectory is T={x.shape[0]} time steps long, so we have to predict T/prediction_offset={x.shape[0]}/{prediction_offset}={length_to_predict} steps because we predict {prediction_offset} steps into the future.\")\n",
    "\n",
    "    limit = length_to_predict\n",
    "    y_pred = np.zeros((limit, 12), dtype=np.float64)\n",
    "    y_pred[0,] = np.concatenate((x[0,], y[0,], vx[0,], vy[0,]))\n",
    "\n",
    "    for i in range(limit - 1):\n",
    "        prediction = model(y_pred[i,].reshape(1, 12), training=False).numpy()\n",
    "\n",
    "        # convert the delta's to an actual prediction\n",
    "        prediction /= scaling_factor\n",
    "        prediction = y_pred[i,].reshape(1, 12) - prediction\n",
    "\n",
    "        # stop early when the system gets out of bounds\n",
    "        if np.min(prediction[0, :6]) < -3 or np.max(prediction[0, :6]) > 3 or np.min(prediction) < -20 or np.max(\n",
    "                prediction) > 20:\n",
    "            print(f\"Stop predicting at t={i * prediction_offset} ({i} steps) after encountering {prediction}\")\n",
    "            break\n",
    "\n",
    "        y_pred[i + 1,] = prediction\n",
    "\n",
    "    y_pred = y_pred[:i]\n",
    "\n",
    "    # Real trajectory\n",
    "    visualize_dataset(*tbp_util.load_dataset(dataset_to_predict), G, M, down_sample_factor=prediction_offset)\n",
    "\n",
    "    # Predicted trajectory\n",
    "    pred_x, pred_y, pred_vx, pred_vy = np.hsplit(y_pred, 4)\n",
    "    visualize_dataset(pred_x, pred_y, pred_vx, pred_vy, G, M, down_sample_factor=1)\n",
    "\n",
    "    # Comparison plot\n",
    "    true_x, true_y, _, _ = tbp_util.load_dataset(dataset_to_predict)\n",
    "    Three_body_2D_Rick.compare_plot(true_x, true_y, pred_x, pred_y, path=f'{model_path}/{model_id}', savefig=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-02T00:50:26.315191400Z",
     "start_time": "2023-07-02T00:49:07.452868Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
